{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T11:15:22.454017Z",
     "start_time": "2025-01-09T11:15:21.643743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from operator import truediv\n",
    "\n",
    "import numpy as np\n",
    "import code_for_hw02_downloadable as helper"
   ],
   "id": "78f2ad61744d9e52",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T11:15:22.479284Z",
     "start_time": "2025-01-09T11:15:22.461028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def perceptron(data, labels, params={}, hook=None):\n",
    "    # if T not in params, default to 100\n",
    "    T = params.get('T', 100)\n",
    "\n",
    "    # Your implementation here\n",
    "    d, n = data.shape\n",
    "    th = np.zeros((d, 1))\n",
    "    th0 = np.zeros((1, 1))\n",
    "    for t in range(T):\n",
    "        for i in range(n):\n",
    "            x = data[:, i:i + 1]\n",
    "            y = labels[:, i:i + 1]\n",
    "            if y * (th.T @ x + th0) <= 0:\n",
    "                th += y * x\n",
    "                th0 += y\n",
    "            if hook:\n",
    "                hook((th, th0))\n",
    "    return th, th0\n",
    "\n",
    "\n",
    "helper.test_perceptron(perceptron)\n",
    "\n"
   ],
   "id": "1488ea35da080d35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Test Perceptron 0-----------\n",
      "Passed! \n",
      "\n",
      "-----------Test Perceptron 1-----------\n",
      "Passed! \n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def averaged_perceptron(data, labels, params={}, hook=None):\n",
    "    # if T not in params, default to 100\n",
    "    T = params.get('T', 100)\n",
    "\n",
    "    # Your implementation here\n",
    "    d, n = data.shape\n",
    "\n",
    "    th = np.zeros((d, 1))\n",
    "    th0 = np.zeros((1, 1))\n",
    "\n",
    "    ths = np.zeros((d,1))\n",
    "    th0s = np.zeros((1,1))\n",
    "    # ths = np.zeros((d, 0))\n",
    "    # th0s = np.zeros((1, 0))\n",
    "\n",
    "    for t in range(T):\n",
    "        for i in range(n):\n",
    "            x = data[:, i:i + 1]\n",
    "            y = labels[:, i:i + 1]\n",
    "            if y * (th.T @ x + th0) <= 0:\n",
    "                th += y * x\n",
    "                th0 += y\n",
    "\n",
    "            ths += th\n",
    "            th0s += th0\n",
    "            # ths = np.concatenate((ths, th), axis=1)\n",
    "            # th0s = np.concatenate((th0s, th0), axis=1)\n",
    "            if hook:\n",
    "                hook((th, th0))\n",
    "    return ths/(n*T), th0s/(n*T)\n",
    "    # return np.mean(ths, axis=1, keepdims=True), np.mean(th0s, axis=1, keepdims=True)\n",
    "\n",
    "helper.test_averaged_perceptron(averaged_perceptron)\n"
   ],
   "id": "643d3ec2a8dc5482",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T12:03:15.730839Z",
     "start_time": "2025-01-09T12:03:15.682145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_classifier(learner, data_train, labels_train, data_test, labels_test):\n",
    "    th, th0 = learner(data_train, labels_train)\n",
    "    score = np.mean(np.sign(th.T @ data_test + th0.T) == labels_test, axis=1, keepdims=True)\n",
    "    return score.item()\n",
    "\n",
    "helper.test_eval_classifier(eval_classifier, perceptron)"
   ],
   "id": "b9211c90e9f9ae0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Test Eval Classifier 0-----------\n",
      "Passed! \n",
      "\n",
      "-----------Test Eval Classifier 1-----------\n",
      "Passed! \n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def eval_learning_alg(learner, data_gen, n_train, n_test, it):\n",
    "    total_score = 0\n",
    "    for i in range(it):\n",
    "        data_train, labels_train = data_gen(n_train)\n",
    "        data_test, labels_test = data_gen(n_test)\n",
    "\n",
    "        total_score += eval_classifier(learner, data_train, labels_train, data_test, labels_test)\n",
    "    return total_score/it\n",
    "\n",
    "helper.test_eval_learning_alg(eval_learning_alg,perceptron)"
   ],
   "id": "dd49ec90dd68ceb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from code_for_hw02_downloadable import score\n",
    "\n",
    "def xval_learning_alg(learner, data, labels, k):\n",
    "    #cross validation of learning algorithm\n",
    "    score_sum = 0\n",
    "    data_list = np.array_split(data, k, axis=1) # data   dxn\n",
    "    labels_list = np.array_split(labels, k, axis=1)   # labels 1xn\n",
    "    for i in range(k):\n",
    "        data_train = np.concatenate([part for j,part in enumerate(data_list) if j !=i], axis=1)\n",
    "        labels_train = np.concatenate([part for j,part in enumerate(labels_list) if j !=i], axis=1)\n",
    "        data_test = data_list[i]\n",
    "        labels_test = labels_list[i]\n",
    "        score_sum += eval_classifier(learner, data_train, labels_train, data_test, labels_test)\n",
    "        # th, th0 = learner(data_train, labels_train)\n",
    "        # score_sum += score(data_test, labels_test, th, th0) / data_test.shape[1]\n",
    "    return score_sum / k\n",
    "\n",
    "helper.test_xval_learning_alg(xval_learning_alg, perceptron)"
   ],
   "id": "4bceb3b7f58898d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for pflip in [0.1, 0.25]:\n",
    "    for alg in [perceptron, averaged_perceptron]:\n",
    "        accuracy = eval_learning_alg(alg, helper.gen_flipped_lin_separable(pflip=pflip), 20, 20,100)\n",
    "        print(pflip, alg.__name__, accuracy)"
   ],
   "id": "2b5f32d9a825dd38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T19:45:27.314415Z",
     "start_time": "2025-01-09T19:45:21.227875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def training_accuracy(learner, data_gen, n_train, it):\n",
    "    total_score = 0\n",
    "    for i in range(it):\n",
    "        data_train, labels_train = data_gen(n_train)\n",
    "        total_score += eval_classifier(learner, data_train, labels_train, data_train, labels_train)\n",
    "    return total_score/it\n",
    "\n",
    "for pflip in [0.1, 0.25]:\n",
    "    for alg in [perceptron, averaged_perceptron]:\n",
    "        accuracy = training_accuracy(alg, helper.gen_flipped_lin_separable(pflip=pflip), 20,100)\n",
    "        print(pflip, alg.__name__, accuracy)\n",
    "\n"
   ],
   "id": "906d4c9e830665dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 perceptron 0.8240000000000001\n",
      "0.1 averaged_perceptron 0.8524999999999995\n",
      "0.25 perceptron 0.6725\n",
      "0.25 averaged_perceptron 0.7169999999999999\n"
     ]
    }
   ],
   "execution_count": 49
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
