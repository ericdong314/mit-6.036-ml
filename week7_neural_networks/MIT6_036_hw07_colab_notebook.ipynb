{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xIaEwCD406A"
   },
   "source": [
    "#MIT 6.036 Spring 2019: Homework 7#\n",
    "\n",
    "This colab notebook provides code and a framework for problem 2 of [the homework](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week7/week7_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
    "\n",
    "## <section>**Setup**</section>\n",
    "\n",
    "First, download the code distribution for this homework that contains test cases and helper functions.\n",
    "\n",
    "Run the next code block to download and import the code for this lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T00:15:58.604475Z",
     "start_time": "2025-02-16T00:15:58.599545Z"
    },
    "id": "2YM-_zLf9Bp-"
   },
   "outputs": [],
   "source": [
    "# !rm -rf code_for_hw7*\n",
    "# !rm -rf mnist\n",
    "# !wget --quiet https://introml_oll.odl.mit.edu/6.036/static/homework/hw07/code_for_hw7.zip\n",
    "# !unzip code_for_hw7.zip\n",
    "# !mv code_for_hw7/* .\n",
    "\n",
    "from code_for_hw7 import *\n",
    "import numpy as np\n",
    "import modules_disp as disp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFxhrJ5XDlvb"
   },
   "source": [
    "# 2) Implementing Neural Networks\n",
    "\n",
    "This homework considers neural networks with multiple layers. Each layer has multiple inputs and outputs, and can be broken down into two parts:\n",
    "\n",
    "<br>\n",
    "A linear module that implements a linear transformation:     $ z_j = (\\sum^{m}_{i=1} x_i W_{i,j}) + {W_0}_jz$\n",
    "\n",
    "specified by a weight matrix $W$ and a bias vector $W_0$. The output is $[z_1, \\ldots, z_n]^T$\n",
    "\n",
    "<br>\n",
    "An activation module that applies an activation function to the outputs of the linear module for some activation function $f$, such as Tanh or ReLU in the hidden layers or Softmax (see below) at the output layer. We write the output as: $[f(z_1), \\ldots, f(z_m)]^T$, although technically, for some activation functions such as softmax, each output will depend on all the $z_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjQgtwPHj08n"
   },
   "source": [
    "We'll use the modular implementation that we guided you through in the previous problem, which leads to clean code. The basic framework for SGD training is given below. We can construct a network and train it as follows:\n",
    "\n",
    "```\n",
    "# build a 3-layer network\n",
    "net = Sequential([Linear(2,3), Tanh(),\n",
    "                  Linear(3,3), Tanh(),\n",
    "    \t          Linear(3,2), SoftMax()])\n",
    "# train the network on data and labels\n",
    "net.sgd(X, Y)\n",
    "```\n",
    "Please fill in any unimplemented methods below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEwpgsbnho9K"
   },
   "source": [
    "## Linear Modules: ##\n",
    "Each linear module has a forward method that takes in a batch of activations A (from the previous layer) and returns a batch of pre-activations Z.\n",
    "\n",
    "Each linear module has a backward method that takes in dLdZ and returns dLdA. This module also computes and stores dLdW and dLdW0, the gradients with respect to the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T00:15:58.647722Z",
     "start_time": "2025-02-16T00:15:58.642058Z"
    },
    "id": "-VsYLAxCfy7U"
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, m, n):\n",
    "        self.m, self.n = (m, n)  # (in size, out size)\n",
    "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
    "        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A  # (m x b)  Hint: make sure you understand what b stands for\n",
    "        return self.W.T @ self.A + self.W0   # Your code (n x b)\n",
    "\n",
    "    def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n",
    "        self.dLdW = self.A @ dLdZ.T  # Your code\n",
    "        self.dLdW0 = dLdZ @ np.ones((dLdZ.shape[1], 1))  # Your code\n",
    "        return self.W @ dLdZ  # Your code: return dLdA (m x b)\n",
    "\n",
    "    def sgd_step(self, lrate):  # Gradient descent step\n",
    "        self.W = self.W - lrate * self.dLdW  # Your code\n",
    "        self.W0 = self.W0 - lrate * self.dLdW0  # Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqZ7_kZYr5s5"
   },
   "source": [
    " You are encouraged to make your own tests for each module. A unit test method and an example test case are given below for your reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T00:15:58.677500Z",
     "start_time": "2025-02-16T00:15:58.669731Z"
    },
    "id": "aY3yePY0r4eA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_forward: OK\n",
      "linear_backward: OK\n",
      "linear_sgd_step_W: OK\n",
      "linear_sgd_step_W0: OK\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# data\n",
    "X, Y = super_simple_separable()\n",
    "\n",
    "# module\n",
    "linear_1 = Linear(2, 3)\n",
    "\n",
    "#hyperparameters\n",
    "lrate = 0.005\n",
    "\n",
    "# test case\n",
    "# forward\n",
    "z_1 = linear_1.forward(X)\n",
    "exp_z_1 = np.array([[10.41750064, 6.91122168, 20.73366505, 22.8912344],\n",
    "                    [7.16872235, 3.48998746, 10.46996239, 9.9982611],\n",
    "                    [-2.07105455, 0.69413716, 2.08241149, 4.84966811]])\n",
    "unit_test(\"linear_forward\", exp_z_1, z_1)\n",
    "\n",
    "# backward\n",
    "dL_dz1 = np.array([[1.69467553e-09, -1.33530535e-06, 0.00000000e+00, -0.00000000e+00],\n",
    "                   [-5.24547376e-07, 5.82459519e-04, -3.84805202e-10, 1.47943038e-09],\n",
    "                   [-3.47063705e-02, 2.55611604e-01, -1.83538094e-02, 1.11838432e-04]])\n",
    "exp_dLdX = np.array([[-2.40194628e-02, 1.77064845e-01, -1.27021626e-02, 7.74006953e-05],\n",
    "                     [2.39827939e-02, -1.75870737e-01, 1.26832126e-02, -7.72828555e-05]])\n",
    "dLdX = linear_1.backward(dL_dz1)\n",
    "unit_test(\"linear_backward\", exp_dLdX, dLdX)\n",
    "\n",
    "# sgd step\n",
    "linear_1.sgd_step(lrate)\n",
    "exp_linear_1_W = np.array([[1.2473734, 0.28294514, 0.68940437],\n",
    "                           [1.58455079, 1.32055711, -0.69218045]]),\n",
    "unit_test(\"linear_sgd_step_W\", exp_linear_1_W, linear_1.W)\n",
    "\n",
    "exp_linear_1_W0 = np.array([[6.66805339e-09],\n",
    "                            [-2.90968033e-06],\n",
    "                            [-1.01331631e-03]]),\n",
    "unit_test(\"linear_sgd_step_W0\", exp_linear_1_W0, linear_1.W0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ETL01mPsBz4"
   },
   "source": [
    "The following datasets are defined for your use:\n",
    "*  `super_simple_separable_through_origin()`\n",
    "*  `super_simple_separable()`\n",
    "*  `xor()`\n",
    "*  `xor_more()`\n",
    "*  `hard()`\n",
    "\n",
    "Further, a plotting function is defined for your usage in modules_disp.py, and can be called in the colab notebook as `disp.plot_nn()`.\n",
    "```\n",
    "def plot_nn(X, Y, nn):\n",
    "    \"\"\" Plot output of nn vs. data \"\"\"\n",
    "    def predict(x):\n",
    "        return nn.modules[-1].class_fun(nn.forward(x))[0]\n",
    "    xmin, ymin = np.min(X, axis=1)-1\n",
    "    xmax, ymax = np.max(X, axis=1)+1\n",
    "    nax = plot_objective_2d(lambda x: predict(x), xmin, xmax, ymin, ymax)\n",
    "    plot_data(X, Y, nax)\n",
    "    plt.show()```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4s70beWJh09h"
   },
   "source": [
    "## Activation functions: ##\n",
    "Each activation module has a forward method that takes in a batch of pre-activations Z and returns a batch of activations A.\n",
    "\n",
    "Each activation module has a backward method that takes in dLdA and returns dLdZ, with the exception of SoftMax, where we assume dLdZ is passed in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwaNAtLnhenT"
   },
   "source": [
    "### Tanh: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T00:15:58.768546Z",
     "start_time": "2025-02-16T00:15:58.761441Z"
    },
    "id": "ff6eD3dnftiR"
   },
   "outputs": [],
   "source": [
    "class Tanh(Module):  # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.tanh(Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):  # Uses stored self.A\n",
    "        return dLdA * (1 - self.A ** 2)  # Your code: return dLdZ (?, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FW7ocKRhcgY"
   },
   "source": [
    "### ReLU: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T00:15:58.837477Z",
     "start_time": "2025-02-16T00:15:58.831372Z"
    },
    "id": "1fm2KsLUfqdp"
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):  # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.where(Z > 0, Z, 0)  # Your code: (?, b)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):  # uses stored self.A\n",
    "        return dLdA * np.where(self.A != 0, 1, 0)  # Your code: return dLdZ (?, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKtXuTQ0hSNO"
   },
   "source": [
    "###SoftMax: ###\n",
    "For `SoftMax.class_fun()`, given the column vector of class probabilities for each point (computed by Softmax), return a vector of the classes (integers) with the highest probability for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T00:15:58.885375Z",
     "start_time": "2025-02-16T00:15:58.879629Z"
    },
    "id": "fqK-CJrnfn22"
   },
   "outputs": [],
   "source": [
    "class SoftMax(Module):  # Output activation\n",
    "    def forward(self, Z):\n",
    "        return np.exp(Z) / np.sum(np.exp(Z), axis=0)  # Your code: (?, b)\n",
    "\n",
    "    def backward(self, dLdZ):  # Assume that dLdZ is passed in\n",
    "        return dLdZ\n",
    "\n",
    "    def class_fun(self, Ypred):  # Return class indices\n",
    "        return np.expand_dims(np.argmax(Ypred, axis=0),0)  # Your code: (1, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZc7HnMSh4fn"
   },
   "source": [
    "## Loss Functions:##\n",
    "Each loss module has a forward method that takes in a batch of predictions Ypred (from the previous layer) and labels Y and returns a scalar loss value.\n",
    "\n",
    "The NLL module has a backward method that returns dLdZ, the gradient with respect to the preactivation to SoftMax (note: not the activation!), since we are always pairing SoftMax activation with NLL loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4uy0pHVhNd8"
   },
   "source": [
    "### NLL: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T00:15:58.917184Z",
     "start_time": "2025-02-16T00:15:58.912924Z"
    },
    "id": "17Fb8mimflgb"
   },
   "outputs": [],
   "source": [
    "class NLL(Module):  # Loss\n",
    "    def forward(self, Ypred, Y):\n",
    "        self.Ypred = Ypred\n",
    "        self.Y = Y\n",
    "        return np.sum(-np.sum(Y * np.log(Ypred), axis=0))  # Your code\n",
    "\n",
    "    def backward(self):  # Use stored self.Ypred, self.Y\n",
    "        return self.Ypred - self.Y  # Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1EffzDFkqMX"
   },
   "source": [
    "## Activation and Loss Test Cases: ##\n",
    "Run Test 1 and Test 2 below and compare your outputs with the expected outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T00:15:58.936318Z",
     "start_time": "2025-02-16T00:15:58.929218Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "9DJFzpahkvcD",
    "outputId": "f37fe4f7-9d34-474f-cac3-2183396e7bed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_1.W: OK\n",
      "linear_1.W0: OK\n",
      "linear_2.W: OK\n",
      "linear_2.W0: OK\n",
      "z_1: OK\n",
      "a_1: OK\n",
      "z_2: OK\n",
      "a_2: OK\n",
      "loss: OK\n",
      "dloss: OK\n",
      "dL_dz2: OK\n",
      "dL_da1: OK\n",
      "dL_dz1: OK\n",
      "dL_dX: OK\n",
      "updated_linear_1.W: OK\n",
      "updated_linear_1.W0: OK\n",
      "updated_linear_2.W: OK\n",
      "updated_linear_2.W0: OK\n"
     ]
    }
   ],
   "source": [
    "# TEST 1: sgd_test for Tanh activation and SoftMax output\n",
    "np.random.seed(0)\n",
    "sgd_test(Sequential([Linear(2, 3), Tanh(), Linear(3, 2), SoftMax()], NLL()), test_1_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T00:15:59.021563Z",
     "start_time": "2025-02-16T00:15:59.015405Z"
    },
    "id": "Bd0dXg-Qk05_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_1.W: OK\n",
      "linear_1.W0: OK\n",
      "linear_2.W: OK\n",
      "linear_2.W0: OK\n",
      "z_1: OK\n",
      "a_1: OK\n",
      "z_2: OK\n",
      "a_2: OK\n",
      "loss: OK\n",
      "dloss: OK\n",
      "dL_dz2: OK\n",
      "dL_da1: OK\n",
      "dL_dz1: OK\n",
      "dL_dX: OK\n",
      "updated_linear_1.W: OK\n",
      "updated_linear_1.W0: OK\n",
      "updated_linear_2.W: OK\n",
      "updated_linear_2.W0: OK\n"
     ]
    }
   ],
   "source": [
    "# TEST 2: sgd_test for ReLU activation and SoftMax output\n",
    "np.random.seed(0)\n",
    "sgd_test(Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL()), test_2_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-l5JgBU2iBCZ"
   },
   "source": [
    "## Neural Network: ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXMGcdnXgiF3"
   },
   "source": [
    "Implement SGD. Randomly pick a data point Xt, Yt by using np.random.randint to choose a random index into the data. Compute the predicted output Ypred for Xt with the forward method. Compute the loss for Ypred relative to Yt. Use the backward method to compute the gradients. Use the sgd_step method to change the weights. Repeat.\n",
    "\n",
    "We will (later) be generalizing SGD to operate on a \"mini-batch\" of data points instead of a single point. You should strive for an implementation of the forward, backward, and `class_fun` methods that works with batches of data. Note that when $b$ is mentioned as part of the shape of a matrix in the code, this $b$ refers to the number of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T00:15:59.108822Z",
     "start_time": "2025-02-16T00:15:59.101076Z"
    },
    "id": "ejO15Vr7fhKB"
   },
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, modules, loss):  # List of modules, loss module\n",
    "        self.modules = modules\n",
    "        self.loss = loss\n",
    "\n",
    "    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n",
    "        D, N = X.shape\n",
    "        for it in range(iters):\n",
    "            i = np.random.randint(0, N)\n",
    "            A = self.forward(X[:,i:i+1])\n",
    "            loss = self.loss.forward(A, Y[:,i:i+1])\n",
    "            err = self.loss.backward()\n",
    "            self.backward(err)\n",
    "            self.sgd_step(lrate)\n",
    "            self.print_accuracy(it,X,Y,loss)\n",
    "\n",
    "    def forward(self, Xt):  # Compute Ypred\n",
    "        for m in self.modules: Xt = m.forward(Xt)\n",
    "        return Xt\n",
    "\n",
    "    def backward(self, delta):  # Update dLdW and dLdW0\n",
    "        # Note reversed list of modules\n",
    "        for m in self.modules[::-1]: delta = m.backward(delta)\n",
    "\n",
    "    def sgd_step(self, lrate):  # Gradient descent step\n",
    "        for m in self.modules: m.sgd_step(lrate)\n",
    "\n",
    "    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n",
    "        # Utility method to print accuracy on full dataset, should\n",
    "        # improve over time when doing SGD. Also prints current loss,\n",
    "        # which should decrease over time. Call this on each iteration\n",
    "        # of SGD!\n",
    "        if it % every == 1:\n",
    "            cf = self.modules[-1].class_fun\n",
    "            acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
    "            print('Iteration =', it, '\\tAcc =', acc, '\\tLoss =', cur_loss, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUojaXqphDjh"
   },
   "source": [
    "## Neural Network / SGD Test Cases: ##\n",
    "Use Test 3 and Test 4 to help you debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T00:16:10.817831Z",
     "start_time": "2025-02-16T00:15:59.205930Z"
    },
    "id": "wmupM8OScodw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 0.5 \tLoss = 0.8163501209126218\n",
      "Iteration = 251 \tAcc = 0.95 \tLoss = 0.10477735930943054\n",
      "Iteration = 501 \tAcc = 0.95 \tLoss = 0.3520196947953326\n",
      "Iteration = 751 \tAcc = 0.95 \tLoss = 0.5078020702186467\n",
      "Iteration = 1001 \tAcc = 0.95 \tLoss = 0.07333365070679529\n",
      "Iteration = 1251 \tAcc = 0.95 \tLoss = 0.02658564276835577\n",
      "Iteration = 1501 \tAcc = 0.95 \tLoss = 0.02471667672374407\n",
      "Iteration = 1751 \tAcc = 0.95 \tLoss = 1.9120151135596022\n",
      "Iteration = 2001 \tAcc = 0.95 \tLoss = 0.010556590138169696\n",
      "Iteration = 2251 \tAcc = 0.95 \tLoss = 0.012015435423861806\n",
      "Iteration = 2501 \tAcc = 0.95 \tLoss = 0.008657021958802665\n",
      "Iteration = 2751 \tAcc = 0.95 \tLoss = 0.008426119188991924\n",
      "Iteration = 3001 \tAcc = 0.95 \tLoss = 0.40637858317403797\n",
      "Iteration = 3251 \tAcc = 0.95 \tLoss = 0.0019107120086166122\n",
      "Iteration = 3501 \tAcc = 0.95 \tLoss = 0.04188944453555038\n",
      "Iteration = 3751 \tAcc = 0.95 \tLoss = 0.003208643240697625\n",
      "Iteration = 4001 \tAcc = 0.95 \tLoss = 0.0027827234467466353\n",
      "Iteration = 4251 \tAcc = 0.95 \tLoss = 0.006447831275421869\n",
      "Iteration = 4501 \tAcc = 0.95 \tLoss = 0.0051098971471632395\n",
      "Iteration = 4751 \tAcc = 0.95 \tLoss = 0.0006015141514427351\n",
      "Iteration = 5001 \tAcc = 0.95 \tLoss = 0.003323513730988572\n",
      "Iteration = 5251 \tAcc = 0.95 \tLoss = 0.002696401140358734\n",
      "Iteration = 5501 \tAcc = 0.95 \tLoss = 1.5733587445769748\n",
      "Iteration = 5751 \tAcc = 0.95 \tLoss = 0.0008904829688640953\n",
      "Iteration = 6001 \tAcc = 0.95 \tLoss = 0.28379989830219554\n",
      "Iteration = 6251 \tAcc = 0.95 \tLoss = 0.0002370717842358095\n",
      "Iteration = 6501 \tAcc = 0.95 \tLoss = 1.6026879551701765\n",
      "Iteration = 6751 \tAcc = 0.95 \tLoss = 0.06348476041514982\n",
      "Iteration = 7001 \tAcc = 0.95 \tLoss = 0.03556668890317227\n",
      "Iteration = 7251 \tAcc = 0.95 \tLoss = 0.0004244692510122907\n",
      "Iteration = 7501 \tAcc = 0.95 \tLoss = 0.000566915875458982\n",
      "Iteration = 7751 \tAcc = 0.95 \tLoss = 0.0016284249399882587\n",
      "Iteration = 8001 \tAcc = 0.95 \tLoss = 1.902562543182756e-05\n",
      "Iteration = 8251 \tAcc = 0.95 \tLoss = 0.0013999772807491162\n",
      "Iteration = 8501 \tAcc = 0.95 \tLoss = 0.023600605117732352\n",
      "Iteration = 8751 \tAcc = 0.95 \tLoss = 0.020988890685233668\n",
      "Iteration = 9001 \tAcc = 0.95 \tLoss = 0.0009944086541207391\n",
      "Iteration = 9251 \tAcc = 0.95 \tLoss = 0.4033352052392005\n",
      "Iteration = 9501 \tAcc = 0.95 \tLoss = 0.01643917092926479\n",
      "Iteration = 9751 \tAcc = 0.95 \tLoss = 0.019482605522865924\n",
      "Iteration = 10001 \tAcc = 0.95 \tLoss = 0.0013002555581601027\n",
      "Iteration = 10251 \tAcc = 0.95 \tLoss = 0.0037168093584231112\n",
      "Iteration = 10501 \tAcc = 0.95 \tLoss = 0.051704508783197774\n",
      "Iteration = 10751 \tAcc = 0.95 \tLoss = 0.21650686261552726\n",
      "Iteration = 11001 \tAcc = 0.95 \tLoss = 0.0009353968413641944\n",
      "Iteration = 11251 \tAcc = 0.95 \tLoss = 0.1838543610478957\n",
      "Iteration = 11501 \tAcc = 0.95 \tLoss = 0.2578018880739477\n",
      "Iteration = 11751 \tAcc = 0.95 \tLoss = 3.5145681645907264e-05\n",
      "Iteration = 12001 \tAcc = 0.95 \tLoss = 0.04832054963963184\n",
      "Iteration = 12251 \tAcc = 0.95 \tLoss = 0.43721734033048615\n",
      "Iteration = 12501 \tAcc = 0.95 \tLoss = 3.819963156900902e-06\n",
      "Iteration = 12751 \tAcc = 0.95 \tLoss = 0.0001525639752614136\n",
      "Iteration = 13001 \tAcc = 0.95 \tLoss = 0.12798685244072414\n",
      "Iteration = 13251 \tAcc = 0.95 \tLoss = 0.02013265757839868\n",
      "Iteration = 13501 \tAcc = 0.95 \tLoss = 0.0003704585858940595\n",
      "Iteration = 13751 \tAcc = 0.95 \tLoss = 0.01977257394891925\n",
      "Iteration = 14001 \tAcc = 0.95 \tLoss = 1.1605467328190633e-05\n",
      "Iteration = 14251 \tAcc = 0.95 \tLoss = 0.16762794634099548\n",
      "Iteration = 14501 \tAcc = 0.95 \tLoss = 0.00032694370044933146\n",
      "Iteration = 14751 \tAcc = 0.95 \tLoss = 0.00036550841233230904\n",
      "Iteration = 15001 \tAcc = 0.95 \tLoss = 0.0002614001534531944\n",
      "Iteration = 15251 \tAcc = 0.95 \tLoss = 0.021407297238818783\n",
      "Iteration = 15501 \tAcc = 0.95 \tLoss = 7.51306911076795e-05\n",
      "Iteration = 15751 \tAcc = 0.95 \tLoss = 0.08944116992361037\n",
      "Iteration = 16001 \tAcc = 0.95 \tLoss = 0.0007504763950901106\n",
      "Iteration = 16251 \tAcc = 0.95 \tLoss = 0.10284490738726118\n",
      "Iteration = 16501 \tAcc = 0.95 \tLoss = 4.298613623249328e-06\n",
      "Iteration = 16751 \tAcc = 0.95 \tLoss = 1.2919460705319854\n",
      "Iteration = 17001 \tAcc = 0.95 \tLoss = 2.50211840360188e-06\n",
      "Iteration = 17251 \tAcc = 0.95 \tLoss = 0.33407517951118637\n",
      "Iteration = 17501 \tAcc = 0.95 \tLoss = 0.0002118630156356675\n",
      "Iteration = 17751 \tAcc = 0.95 \tLoss = 0.00020569750935814878\n",
      "Iteration = 18001 \tAcc = 0.95 \tLoss = 0.00020029163864487923\n",
      "Iteration = 18251 \tAcc = 0.95 \tLoss = 2.4444760649052847e-05\n",
      "Iteration = 18501 \tAcc = 0.95 \tLoss = 2.778742282727566e-06\n",
      "Iteration = 18751 \tAcc = 0.95 \tLoss = 1.303889203904021e-07\n",
      "Iteration = 19001 \tAcc = 0.95 \tLoss = 0.00013892921656429846\n",
      "Iteration = 19251 \tAcc = 0.95 \tLoss = 0.09422176785103047\n",
      "Iteration = 19501 \tAcc = 0.95 \tLoss = 0.00011630084144658905\n",
      "Iteration = 19751 \tAcc = 0.95 \tLoss = 0.004786035510234691\n",
      "Iteration = 20001 \tAcc = 0.95 \tLoss = 0.9370938733945082\n",
      "Iteration = 20251 \tAcc = 0.95 \tLoss = 0.10584146383866073\n",
      "Iteration = 20501 \tAcc = 0.95 \tLoss = 9.764856072535319e-05\n",
      "Iteration = 20751 \tAcc = 0.95 \tLoss = 0.00012020331694157445\n",
      "Iteration = 21001 \tAcc = 0.95 \tLoss = 0.000828009986534501\n",
      "Iteration = 21251 \tAcc = 0.95 \tLoss = 0.006506187099757169\n",
      "Iteration = 21501 \tAcc = 0.95 \tLoss = 4.2104380526559754e-05\n",
      "Iteration = 21751 \tAcc = 0.95 \tLoss = 0.00010465490697483777\n",
      "Iteration = 22001 \tAcc = 0.95 \tLoss = 3.713281471519605e-05\n",
      "Iteration = 22251 \tAcc = 1.0 \tLoss = 0.22262267109855993\n",
      "Iteration = 22501 \tAcc = 0.95 \tLoss = 7.061355965640761e-05\n",
      "Iteration = 22751 \tAcc = 0.95 \tLoss = 1.8868229490301634e-08\n",
      "Iteration = 23001 \tAcc = 0.95 \tLoss = 6.572622370549784e-05\n",
      "Iteration = 23251 \tAcc = 0.95 \tLoss = 0.36605450710160453\n",
      "Iteration = 23501 \tAcc = 1.0 \tLoss = 0.7850514102745721\n",
      "Iteration = 23751 \tAcc = 0.95 \tLoss = 1.1493024316632003e-09\n",
      "Iteration = 24001 \tAcc = 0.95 \tLoss = 0.08887984467325091\n",
      "Iteration = 24251 \tAcc = 0.95 \tLoss = 1.0022895896846751e-08\n",
      "Iteration = 24501 \tAcc = 0.95 \tLoss = 0.07104267499365308\n",
      "Iteration = 24751 \tAcc = 0.95 \tLoss = 0.059653702215375144\n",
      "Iteration = 25001 \tAcc = 0.95 \tLoss = 8.214733983649385e-06\n",
      "Iteration = 25251 \tAcc = 0.95 \tLoss = 0.031103845041374163\n",
      "Iteration = 25501 \tAcc = 0.95 \tLoss = 2.912636430932822e-06\n",
      "Iteration = 25751 \tAcc = 0.95 \tLoss = 4.782713236896818e-09\n",
      "Iteration = 26001 \tAcc = 1.0 \tLoss = 0.6937209232582289\n",
      "Iteration = 26251 \tAcc = 1.0 \tLoss = 0.013726748523643565\n",
      "Iteration = 26501 \tAcc = 1.0 \tLoss = 5.710283743100057e-05\n",
      "Iteration = 26751 \tAcc = 0.95 \tLoss = 6.0378400066182605e-05\n",
      "Iteration = 27001 \tAcc = 0.95 \tLoss = 0.002198021926355266\n",
      "Iteration = 27251 \tAcc = 0.95 \tLoss = 0.006365730424019206\n",
      "Iteration = 27501 \tAcc = 1.0 \tLoss = 0.6859455806964175\n",
      "Iteration = 27751 \tAcc = 0.95 \tLoss = 5.425935916514706e-05\n",
      "Iteration = 28001 \tAcc = 0.95 \tLoss = 3.999287538466397e-05\n",
      "Iteration = 28251 \tAcc = 1.0 \tLoss = 6.8444033521357535e-06\n",
      "Iteration = 28501 \tAcc = 0.95 \tLoss = 1.051912813779129e-06\n",
      "Iteration = 28751 \tAcc = 0.95 \tLoss = 0.003291734650309549\n",
      "Iteration = 29001 \tAcc = 0.95 \tLoss = 0.4302422582984554\n",
      "Iteration = 29251 \tAcc = 1.0 \tLoss = 4.372503500408951e-06\n",
      "Iteration = 29501 \tAcc = 1.0 \tLoss = 1.7817785551128628e-06\n",
      "Iteration = 29751 \tAcc = 1.0 \tLoss = 2.020428719693071e-06\n",
      "Iteration = 30001 \tAcc = 0.95 \tLoss = 0.4460975756749418\n",
      "Iteration = 30251 \tAcc = 0.95 \tLoss = 0.32999634244824266\n",
      "Iteration = 30501 \tAcc = 0.95 \tLoss = 6.866958552539728e-07\n",
      "Iteration = 30751 \tAcc = 0.95 \tLoss = 6.993380321732125e-10\n",
      "Iteration = 31001 \tAcc = 1.0 \tLoss = 0.04181892784859271\n",
      "Iteration = 31251 \tAcc = 0.95 \tLoss = 2.5122635569976227e-06\n",
      "Iteration = 31501 \tAcc = 1.0 \tLoss = 1.9059531730829084e-11\n",
      "Iteration = 31751 \tAcc = 0.95 \tLoss = 4.0116965191932794e-05\n",
      "Iteration = 32001 \tAcc = 0.95 \tLoss = 8.598757409771227e-09\n",
      "Iteration = 32251 \tAcc = 0.95 \tLoss = 8.971486767231688e-06\n",
      "Iteration = 32501 \tAcc = 0.95 \tLoss = 1.4664491843171366e-11\n",
      "Iteration = 32751 \tAcc = 0.95 \tLoss = 0.020227143299616758\n",
      "Iteration = 33001 \tAcc = 1.0 \tLoss = 3.0217128803702447e-10\n",
      "Iteration = 33251 \tAcc = 1.0 \tLoss = 1.2312599517126982e-06\n",
      "Iteration = 33501 \tAcc = 0.95 \tLoss = 3.198947939298157e-05\n",
      "Iteration = 33751 \tAcc = 1.0 \tLoss = 5.719257574335866e-07\n",
      "Iteration = 34001 \tAcc = 0.95 \tLoss = 2.9491418501080994e-05\n",
      "Iteration = 34251 \tAcc = 0.95 \tLoss = 7.412757890023191e-06\n",
      "Iteration = 34501 \tAcc = 0.95 \tLoss = 0.0002197508350075361\n",
      "Iteration = 34751 \tAcc = 0.95 \tLoss = 3.8058859034693666e-07\n",
      "Iteration = 35001 \tAcc = 1.0 \tLoss = 5.077999256212236e-09\n",
      "Iteration = 35251 \tAcc = 0.95 \tLoss = 0.0011234321536105585\n",
      "Iteration = 35501 \tAcc = 1.0 \tLoss = 1.5800616372101802e-10\n",
      "Iteration = 35751 \tAcc = 1.0 \tLoss = 3.930078484878314e-12\n",
      "Iteration = 36001 \tAcc = 1.0 \tLoss = 0.025396126562211685\n",
      "Iteration = 36251 \tAcc = 1.0 \tLoss = 0.0016810909847229582\n",
      "Iteration = 36501 \tAcc = 1.0 \tLoss = 4.211366601802077e-05\n",
      "Iteration = 36751 \tAcc = 1.0 \tLoss = 0.0013700582912151622\n",
      "Iteration = 37001 \tAcc = 1.0 \tLoss = 0.008248420594466968\n",
      "Iteration = 37251 \tAcc = 0.95 \tLoss = 3.581292960648546e-07\n",
      "Iteration = 37501 \tAcc = 1.0 \tLoss = 0.0015455309223089968\n",
      "Iteration = 37751 \tAcc = 1.0 \tLoss = 0.0010187469485047082\n",
      "Iteration = 38001 \tAcc = 1.0 \tLoss = 0.43626755964324904\n",
      "Iteration = 38251 \tAcc = 1.0 \tLoss = 0.0007356998088186914\n",
      "Iteration = 38501 \tAcc = 1.0 \tLoss = 9.5652673711212e-06\n",
      "Iteration = 38751 \tAcc = 0.95 \tLoss = 3.9729441959302904e-11\n",
      "Iteration = 39001 \tAcc = 0.95 \tLoss = 1.5657415376502986e-09\n",
      "Iteration = 39251 \tAcc = 1.0 \tLoss = 0.0021560085411305033\n",
      "Iteration = 39501 \tAcc = 0.95 \tLoss = 0.0003030627939025226\n",
      "Iteration = 39751 \tAcc = 1.0 \tLoss = 0.0014113418192738462\n",
      "Iteration = 40001 \tAcc = 1.0 \tLoss = 0.0015012958849374171\n",
      "Iteration = 40251 \tAcc = 1.0 \tLoss = 1.946165451035606e-11\n",
      "Iteration = 40501 \tAcc = 1.0 \tLoss = 0.001813305740016457\n",
      "Iteration = 40751 \tAcc = 1.0 \tLoss = 4.0239998765308026e-05\n",
      "Iteration = 41001 \tAcc = 1.0 \tLoss = 0.8066443293882426\n",
      "Iteration = 41251 \tAcc = 1.0 \tLoss = 4.139709098764842e-05\n",
      "Iteration = 41501 \tAcc = 1.0 \tLoss = 0.01291038331312912\n",
      "Iteration = 41751 \tAcc = 1.0 \tLoss = 0.6089258117517168\n",
      "Iteration = 42001 \tAcc = 1.0 \tLoss = 1.2239818935363735e-05\n",
      "Iteration = 42251 \tAcc = 1.0 \tLoss = 9.89801491028658e-06\n",
      "Iteration = 42501 \tAcc = 1.0 \tLoss = 0.0007115358024604936\n",
      "Iteration = 42751 \tAcc = 1.0 \tLoss = 0.366184651353865\n",
      "Iteration = 43001 \tAcc = 0.95 \tLoss = 2.300382107023589e-13\n",
      "Iteration = 43251 \tAcc = 1.0 \tLoss = 1.2626421027817004e-09\n",
      "Iteration = 43501 \tAcc = 0.95 \tLoss = 0.0002542366541648453\n",
      "Iteration = 43751 \tAcc = 1.0 \tLoss = 0.01423118721577605\n",
      "Iteration = 44001 \tAcc = 1.0 \tLoss = 7.260858581048788e-14\n",
      "Iteration = 44251 \tAcc = 1.0 \tLoss = 5.677902592554095e-12\n",
      "Iteration = 44501 \tAcc = 1.0 \tLoss = 4.9960036108133294e-14\n",
      "Iteration = 44751 \tAcc = 0.95 \tLoss = 1.8308907534158024e-07\n",
      "Iteration = 45001 \tAcc = 1.0 \tLoss = 0.014306832845749096\n",
      "Iteration = 45251 \tAcc = 1.0 \tLoss = 4.2409327169395884e-05\n",
      "Iteration = 45501 \tAcc = 1.0 \tLoss = 0.0010050898375812399\n",
      "Iteration = 45751 \tAcc = 1.0 \tLoss = 3.173133117757271e-08\n",
      "Iteration = 46001 \tAcc = 1.0 \tLoss = 0.0061128991710091484\n",
      "Iteration = 46251 \tAcc = 0.95 \tLoss = 0.0037640267873350476\n",
      "Iteration = 46501 \tAcc = 0.95 \tLoss = 0.18548645502022165\n",
      "Iteration = 46751 \tAcc = 1.0 \tLoss = 1.0782852248462173e-07\n",
      "Iteration = 47001 \tAcc = 1.0 \tLoss = 5.183529162800039e-10\n",
      "Iteration = 47251 \tAcc = 1.0 \tLoss = 0.35640729246623476\n",
      "Iteration = 47501 \tAcc = 0.95 \tLoss = 9.314376390416964e-07\n",
      "Iteration = 47751 \tAcc = 0.95 \tLoss = 0.007514261797095945\n",
      "Iteration = 48001 \tAcc = 1.0 \tLoss = 4.31489557246605e-05\n",
      "Iteration = 48251 \tAcc = 1.0 \tLoss = 0.007958062125005173\n",
      "Iteration = 48501 \tAcc = 0.95 \tLoss = 4.6900223105734504e-05\n",
      "Iteration = 48751 \tAcc = 1.0 \tLoss = 0.0035765914129995313\n",
      "Iteration = 49001 \tAcc = 1.0 \tLoss = 1.5737144921773973e-10\n",
      "Iteration = 49251 \tAcc = 1.0 \tLoss = 3.993827446779499e-05\n",
      "Iteration = 49501 \tAcc = 1.0 \tLoss = 2.849359833150329e-08\n",
      "Iteration = 49751 \tAcc = 1.0 \tLoss = 4.697587080020128e-06\n",
      "Iteration = 50001 \tAcc = 1.0 \tLoss = 3.9950660906714015e-05\n",
      "Iteration = 50251 \tAcc = 1.0 \tLoss = 0.7229072481221674\n",
      "Iteration = 50501 \tAcc = 1.0 \tLoss = 1.6358026044840436e-12\n",
      "Iteration = 50751 \tAcc = 1.0 \tLoss = 0.013827696896923336\n",
      "Iteration = 51001 \tAcc = 1.0 \tLoss = 0.24554175918068877\n",
      "Iteration = 51251 \tAcc = 1.0 \tLoss = 1.3593570713519656e-12\n",
      "Iteration = 51501 \tAcc = 1.0 \tLoss = 0.0009242725736580308\n",
      "Iteration = 51751 \tAcc = 0.95 \tLoss = 0.002156266929228515\n",
      "Iteration = 52001 \tAcc = 1.0 \tLoss = 3.189873843128527e-07\n",
      "Iteration = 52251 \tAcc = 1.0 \tLoss = 0.008261314017202356\n",
      "Iteration = 52501 \tAcc = 1.0 \tLoss = 2.665747194831931e-05\n",
      "Iteration = 52751 \tAcc = 1.0 \tLoss = 0.0022469262767861138\n",
      "Iteration = 53001 \tAcc = 1.0 \tLoss = 0.0023488552236422015\n",
      "Iteration = 53251 \tAcc = 1.0 \tLoss = 0.4541083684969353\n",
      "Iteration = 53501 \tAcc = 0.95 \tLoss = 0.007785556969309309\n",
      "Iteration = 53751 \tAcc = 1.0 \tLoss = 1.4863375580248395e-07\n",
      "Iteration = 54001 \tAcc = 1.0 \tLoss = 0.0013721354636654747\n",
      "Iteration = 54251 \tAcc = 1.0 \tLoss = 0.0009308598725705525\n",
      "Iteration = 54501 \tAcc = 1.0 \tLoss = 1.042885092967262e-06\n",
      "Iteration = 54751 \tAcc = 1.0 \tLoss = 0.004338064061743084\n",
      "Iteration = 55001 \tAcc = 0.95 \tLoss = 0.12452119562990413\n",
      "Iteration = 55251 \tAcc = 1.0 \tLoss = 2.2204460492503154e-15\n",
      "Iteration = 55501 \tAcc = 1.0 \tLoss = 0.0019473145611205093\n",
      "Iteration = 55751 \tAcc = 1.0 \tLoss = 2.0252029143794506e-05\n",
      "Iteration = 56001 \tAcc = 1.0 \tLoss = 1.0688117058637562e-10\n",
      "Iteration = 56251 \tAcc = 1.0 \tLoss = 3.90909526970594e-13\n",
      "Iteration = 56501 \tAcc = 1.0 \tLoss = 0.00180345973947054\n",
      "Iteration = 56751 \tAcc = 1.0 \tLoss = 1.1772448515786992e-05\n",
      "Iteration = 57001 \tAcc = 1.0 \tLoss = 2.1094237467877998e-15\n",
      "Iteration = 57251 \tAcc = 1.0 \tLoss = 7.75854579592652e-08\n",
      "Iteration = 57501 \tAcc = 1.0 \tLoss = 6.823963616630864e-11\n",
      "Iteration = 57751 \tAcc = 1.0 \tLoss = 1.0544011666030708e-05\n",
      "Iteration = 58001 \tAcc = 1.0 \tLoss = 6.65173471880916e-11\n",
      "Iteration = 58251 \tAcc = 1.0 \tLoss = 0.004751373339524853\n",
      "Iteration = 58501 \tAcc = 1.0 \tLoss = 1.9047752722666735e-06\n",
      "Iteration = 58751 \tAcc = 1.0 \tLoss = 2.177147351290169e-13\n",
      "Iteration = 59001 \tAcc = 1.0 \tLoss = 4.850253532258039e-11\n",
      "Iteration = 59251 \tAcc = 1.0 \tLoss = 0.0029838803609362435\n",
      "Iteration = 59501 \tAcc = 1.0 \tLoss = 0.0014905574804381866\n",
      "Iteration = 59751 \tAcc = 1.0 \tLoss = 0.00517841086925916\n",
      "Iteration = 60001 \tAcc = 1.0 \tLoss = 0.0016644264524147436\n",
      "Iteration = 60251 \tAcc = 1.0 \tLoss = 2.236044450099894e-06\n",
      "Iteration = 60501 \tAcc = 1.0 \tLoss = 1.1066952556137242e-07\n",
      "Iteration = 60751 \tAcc = 1.0 \tLoss = 0.0013027705741328727\n",
      "Iteration = 61001 \tAcc = 1.0 \tLoss = 0.0010409359537286028\n",
      "Iteration = 61251 \tAcc = 1.0 \tLoss = 9.993653149230015e-08\n",
      "Iteration = 61501 \tAcc = 1.0 \tLoss = 0.009896086650195387\n",
      "Iteration = 61751 \tAcc = 1.0 \tLoss = 7.039884406839752e-06\n",
      "Iteration = 62001 \tAcc = 1.0 \tLoss = 0.0014695697197052238\n",
      "Iteration = 62251 \tAcc = 1.0 \tLoss = 4.440892098500627e-16\n",
      "Iteration = 62501 \tAcc = 1.0 \tLoss = 0.0003827587068795779\n",
      "Iteration = 62751 \tAcc = 1.0 \tLoss = 4.861456200307951e-08\n",
      "Iteration = 63001 \tAcc = 1.0 \tLoss = 1.1309440257651746e-07\n",
      "Iteration = 63251 \tAcc = 1.0 \tLoss = 0.0046292932837122495\n",
      "Iteration = 63501 \tAcc = 1.0 \tLoss = 7.509104449382902e-12\n",
      "Iteration = 63751 \tAcc = 1.0 \tLoss = 0.0006277268133317588\n",
      "Iteration = 64001 \tAcc = 1.0 \tLoss = 0.008162399398192115\n",
      "Iteration = 64251 \tAcc = 1.0 \tLoss = 6.422973264384545e-12\n",
      "Iteration = 64501 \tAcc = 1.0 \tLoss = 3.561761216112411e-08\n",
      "Iteration = 64751 \tAcc = 1.0 \tLoss = 5.51780843238718e-14\n",
      "Iteration = 65001 \tAcc = 1.0 \tLoss = 0.0009249051014270705\n",
      "Iteration = 65251 \tAcc = 1.0 \tLoss = 6.875550276679318e-06\n",
      "Iteration = 65501 \tAcc = 1.0 \tLoss = 0.004324894790230053\n",
      "Iteration = 65751 \tAcc = 1.0 \tLoss = 6.026290577683508e-12\n",
      "Iteration = 66001 \tAcc = 1.0 \tLoss = 5.561950240597211e-07\n",
      "Iteration = 66251 \tAcc = 1.0 \tLoss = 1.350267584644476e-08\n",
      "Iteration = 66501 \tAcc = 1.0 \tLoss = 4.6719295099360345e-12\n",
      "Iteration = 66751 \tAcc = 1.0 \tLoss = 6.095536308752169e-06\n",
      "Iteration = 67001 \tAcc = 1.0 \tLoss = 0.0012593515043154014\n",
      "Iteration = 67251 \tAcc = 1.0 \tLoss = 6.306947988307784e-06\n",
      "Iteration = 67501 \tAcc = 1.0 \tLoss = 1.1102230246251565e-16\n",
      "Iteration = 67751 \tAcc = 1.0 \tLoss = 2.2204460492503136e-16\n",
      "Iteration = 68001 \tAcc = 1.0 \tLoss = 4.4695771547080436e-07\n",
      "Iteration = 68251 \tAcc = 1.0 \tLoss = 0.007302217994855475\n",
      "Iteration = 68501 \tAcc = 1.0 \tLoss = 0.0005997285178261575\n",
      "Iteration = 68751 \tAcc = 1.0 \tLoss = 0.001980494361242279\n",
      "Iteration = 69001 \tAcc = 1.0 \tLoss = 5.583842125321862e-06\n",
      "Iteration = 69251 \tAcc = 1.0 \tLoss = 2.588237132056056e-07\n",
      "Iteration = 69501 \tAcc = 1.0 \tLoss = 0.27892350264317595\n",
      "Iteration = 69751 \tAcc = 1.0 \tLoss = 0.0032840539195771556\n",
      "Iteration = 70001 \tAcc = 1.0 \tLoss = 2.4868995751603816e-14\n",
      "Iteration = 70251 \tAcc = 1.0 \tLoss = 0.0011258814571703667\n",
      "Iteration = 70501 \tAcc = 1.0 \tLoss = 1.1102230246251565e-16\n",
      "Iteration = 70751 \tAcc = 1.0 \tLoss = 0.0010251835510355091\n",
      "Iteration = 71001 \tAcc = 1.0 \tLoss = 2.4646951146678778e-14\n",
      "Iteration = 71251 \tAcc = 1.0 \tLoss = 8.654299499293005e-12\n",
      "Iteration = 71501 \tAcc = 1.0 \tLoss = 1.6542323066914968e-14\n",
      "Iteration = 71751 \tAcc = 1.0 \tLoss = 0.002370929170439777\n",
      "Iteration = 72001 \tAcc = 1.0 \tLoss = 8.106444407917651e-10\n",
      "Iteration = 72251 \tAcc = 1.0 \tLoss = 4.527736155161061e-06\n",
      "Iteration = 72501 \tAcc = 1.0 \tLoss = 0.0034166220697239933\n",
      "Iteration = 72751 \tAcc = 1.0 \tLoss = 0.19071599638142903\n",
      "Iteration = 73001 \tAcc = 1.0 \tLoss = 0.09590698676329354\n",
      "Iteration = 73251 \tAcc = 1.0 \tLoss = 0.0009029406695642633\n",
      "Iteration = 73501 \tAcc = 1.0 \tLoss = 0.09918234011652291\n",
      "Iteration = 73751 \tAcc = 1.0 \tLoss = 1.3766765505352036e-14\n",
      "Iteration = 74001 \tAcc = 1.0 \tLoss = 0.10317661175075139\n",
      "Iteration = 74251 \tAcc = 1.0 \tLoss = 4.42424381020031e-06\n",
      "Iteration = 74501 \tAcc = 1.0 \tLoss = 0.001353761413516696\n",
      "Iteration = 74751 \tAcc = 1.0 \tLoss = 6.971266899512201e-10\n",
      "Iteration = 75001 \tAcc = 1.0 \tLoss = 2.481357288855613e-07\n",
      "Iteration = 75251 \tAcc = 1.0 \tLoss = 3.3359992958076328e-06\n",
      "Iteration = 75501 \tAcc = 1.0 \tLoss = 0.0012890115695565754\n",
      "Iteration = 75751 \tAcc = 1.0 \tLoss = 0.0010021609683224955\n",
      "Iteration = 76001 \tAcc = 1.0 \tLoss = 1.0794698468436223e-12\n",
      "Iteration = 76251 \tAcc = 1.0 \tLoss = 3.0116466947703683e-06\n",
      "Iteration = 76501 \tAcc = 1.0 \tLoss = 4.801391532798269e-06\n",
      "Iteration = 76751 \tAcc = 1.0 \tLoss = 0.1015426334134511\n",
      "Iteration = 77001 \tAcc = 1.0 \tLoss = 0.0008779337810874623\n",
      "Iteration = 77251 \tAcc = 1.0 \tLoss = 0.0925030622222877\n",
      "Iteration = 77501 \tAcc = 1.0 \tLoss = 3.51751960892607e-12\n",
      "Iteration = 77751 \tAcc = 1.0 \tLoss = 0.0014453304366853874\n",
      "Iteration = 78001 \tAcc = 1.0 \tLoss = 1.537698058494497e-08\n",
      "Iteration = 78251 \tAcc = 1.0 \tLoss = 0.00022864387222386347\n",
      "Iteration = 78501 \tAcc = 1.0 \tLoss = 0.0008045971198480724\n",
      "Iteration = 78751 \tAcc = 1.0 \tLoss = 0.005819183612008976\n",
      "Iteration = 79001 \tAcc = 1.0 \tLoss = 8.854923275875818e-08\n",
      "Iteration = 79251 \tAcc = 1.0 \tLoss = 0.002708183510087148\n",
      "Iteration = 79501 \tAcc = 1.0 \tLoss = 5.440092820663282e-15\n",
      "Iteration = 79751 \tAcc = 1.0 \tLoss = 5.1070259132757335e-15\n",
      "Iteration = 80001 \tAcc = 1.0 \tLoss = 0.005441250999549882\n",
      "Iteration = 80251 \tAcc = 1.0 \tLoss = 3.9968028886505714e-15\n",
      "Iteration = 80501 \tAcc = 1.0 \tLoss = 2.70950956747629e-06\n",
      "Iteration = 80751 \tAcc = 1.0 \tLoss = 0.005143179536972194\n",
      "Iteration = 81001 \tAcc = 1.0 \tLoss = 0.0013627807595492998\n",
      "Iteration = 81251 \tAcc = 1.0 \tLoss = 4.067034906842552e-06\n",
      "Iteration = 81501 \tAcc = 1.0 \tLoss = 3.2667135668524836e-10\n",
      "Iteration = 81751 \tAcc = 1.0 \tLoss = 2.594136156901343e-06\n",
      "Iteration = 82001 \tAcc = 1.0 \tLoss = 0.0006849329873448129\n",
      "Iteration = 82251 \tAcc = 1.0 \tLoss = 0.0010832946146492961\n",
      "Iteration = 82501 \tAcc = 1.0 \tLoss = 5.987432771805262e-13\n",
      "Iteration = 82751 \tAcc = 1.0 \tLoss = 0.002069960998907038\n",
      "Iteration = 83001 \tAcc = 1.0 \tLoss = 6.55808740646295e-13\n",
      "Iteration = 83251 \tAcc = 1.0 \tLoss = 0.00047734781393312173\n",
      "Iteration = 83501 \tAcc = 1.0 \tLoss = 4.037938073779876e-06\n",
      "Iteration = 83751 \tAcc = 1.0 \tLoss = 5.129625811343674e-08\n",
      "Iteration = 84001 \tAcc = 1.0 \tLoss = 2.2724934182491562e-09\n",
      "Iteration = 84251 \tAcc = 1.0 \tLoss = 2.7191116029145775e-10\n",
      "Iteration = 84501 \tAcc = 1.0 \tLoss = 1.1197973809026353e-07\n",
      "Iteration = 84751 \tAcc = 1.0 \tLoss = 0.004952140466232936\n",
      "Iteration = 85001 \tAcc = 1.0 \tLoss = 0.00048380345165363787\n",
      "Iteration = 85251 \tAcc = 1.0 \tLoss = 0.0006204743528660781\n",
      "Iteration = 85501 \tAcc = 1.0 \tLoss = 4.0360998943001165e-06\n",
      "Iteration = 85751 \tAcc = 1.0 \tLoss = 2.5535129566378632e-15\n",
      "Iteration = 86001 \tAcc = 1.0 \tLoss = 0.0\n",
      "Iteration = 86251 \tAcc = 1.0 \tLoss = 1.2639889135365396e-12\n",
      "Iteration = 86501 \tAcc = 1.0 \tLoss = 2.3314683517128315e-15\n",
      "Iteration = 86751 \tAcc = 1.0 \tLoss = 0.00046668346051444355\n",
      "Iteration = 87001 \tAcc = 1.0 \tLoss = 2.5535129566378632e-15\n",
      "Iteration = 87251 \tAcc = 1.0 \tLoss = 1.991466105118022e-09\n",
      "Iteration = 87501 \tAcc = 1.0 \tLoss = 3.7333836508706943e-06\n",
      "Iteration = 87751 \tAcc = 1.0 \tLoss = 0.0007001569430026693\n",
      "Iteration = 88001 \tAcc = 1.0 \tLoss = 0.0005434373985479592\n",
      "Iteration = 88251 \tAcc = 1.0 \tLoss = 1.0621503676594513e-12\n",
      "Iteration = 88501 \tAcc = 1.0 \tLoss = 9.406919687653376e-13\n",
      "Iteration = 88751 \tAcc = 1.0 \tLoss = 1.4937755488163757e-09\n",
      "Iteration = 89001 \tAcc = 1.0 \tLoss = 2.54883457816102e-06\n",
      "Iteration = 89251 \tAcc = 1.0 \tLoss = 0.0012786824787569184\n",
      "Iteration = 89501 \tAcc = 1.0 \tLoss = 0.0004987300839008301\n",
      "Iteration = 89751 \tAcc = 1.0 \tLoss = 2.652153748275904e-06\n",
      "Iteration = 90001 \tAcc = 1.0 \tLoss = 2.3578157232236705e-09\n",
      "Iteration = 90251 \tAcc = 1.0 \tLoss = 9.893197372439663e-13\n",
      "Iteration = 90501 \tAcc = 1.0 \tLoss = 0.0021047387783626583\n",
      "Iteration = 90751 \tAcc = 1.0 \tLoss = 3.370637102762543e-13\n",
      "Iteration = 91001 \tAcc = 1.0 \tLoss = 1.457104105102783e-09\n",
      "Iteration = 91251 \tAcc = 1.0 \tLoss = 0.1389572849369769\n",
      "Iteration = 91501 \tAcc = 1.0 \tLoss = 2.685470627259306e-09\n",
      "Iteration = 91751 \tAcc = 1.0 \tLoss = 2.837060338044621e-08\n",
      "Iteration = 92001 \tAcc = 1.0 \tLoss = 0.0018137161381534613\n",
      "Iteration = 92251 \tAcc = 1.0 \tLoss = 0.0016867546813186713\n",
      "Iteration = 92501 \tAcc = 1.0 \tLoss = 0.0\n",
      "Iteration = 92751 \tAcc = 1.0 \tLoss = 1.6653345369377362e-15\n",
      "Iteration = 93001 \tAcc = 1.0 \tLoss = 0.001200175911849465\n",
      "Iteration = 93251 \tAcc = 1.0 \tLoss = 1.221245327087673e-15\n",
      "Iteration = 93501 \tAcc = 1.0 \tLoss = 0.0\n",
      "Iteration = 93751 \tAcc = 1.0 \tLoss = 3.500238677073903e-06\n",
      "Iteration = 94001 \tAcc = 1.0 \tLoss = 3.854651344660781e-06\n",
      "Iteration = 94251 \tAcc = 1.0 \tLoss = 2.494138720071072e-06\n",
      "Iteration = 94501 \tAcc = 1.0 \tLoss = 5.948574965943358e-13\n",
      "Iteration = 94751 \tAcc = 1.0 \tLoss = 1.5895773598927908e-09\n",
      "Iteration = 95001 \tAcc = 1.0 \tLoss = 0.0010405483567352343\n",
      "Iteration = 95251 \tAcc = 1.0 \tLoss = 3.135236741810163e-09\n",
      "Iteration = 95501 \tAcc = 1.0 \tLoss = 0.09676040285218286\n",
      "Iteration = 95751 \tAcc = 1.0 \tLoss = 2.2435351379655627e-09\n",
      "Iteration = 96001 \tAcc = 1.0 \tLoss = 3.3246301082119584e-06\n",
      "Iteration = 96251 \tAcc = 1.0 \tLoss = 0.0017265081373818638\n",
      "Iteration = 96501 \tAcc = 1.0 \tLoss = 2.2723878259694422e-08\n",
      "Iteration = 96751 \tAcc = 1.0 \tLoss = 0.06265590270929415\n",
      "Iteration = 97001 \tAcc = 1.0 \tLoss = 0.00041110063700115475\n",
      "Iteration = 97251 \tAcc = 1.0 \tLoss = 2.181718994871212e-06\n",
      "Iteration = 97501 \tAcc = 1.0 \tLoss = 0.0010491215680360534\n",
      "Iteration = 97751 \tAcc = 1.0 \tLoss = 1.3731504916366959e-08\n",
      "Iteration = 98001 \tAcc = 1.0 \tLoss = 0.003291073885935593\n",
      "Iteration = 98251 \tAcc = 1.0 \tLoss = 0.000502985130094633\n",
      "Iteration = 98501 \tAcc = 1.0 \tLoss = 0.00046890738224997827\n",
      "Iteration = 98751 \tAcc = 1.0 \tLoss = 0.0010887329272437287\n",
      "Iteration = 99001 \tAcc = 1.0 \tLoss = 0.00039561820532244625\n",
      "Iteration = 99251 \tAcc = 1.0 \tLoss = 2.5580052866778616e-06\n",
      "Iteration = 99501 \tAcc = 1.0 \tLoss = 1.2030468915715331e-08\n",
      "Iteration = 99751 \tAcc = 1.0 \tLoss = 0.0004021913461515541\n",
      "-3.46493986 -3.41956036 3.39710997 2.0597278\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Sequential at 0x184e272e240>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST 3: you should achieve 100% accuracy on the hard dataset (note that we provided plotting code)\n",
    "X, Y = hard()\n",
    "nn = Sequential([Linear(2, 10), ReLU(), Linear(10, 10), ReLU(), Linear(10, 2), SoftMax()], NLL())\n",
    "disp.classify(X, Y, nn, it=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T00:16:12.738165Z",
     "start_time": "2025-02-16T00:16:12.728649Z"
    },
    "id": "MaWfgC7Qe3ar"
   },
   "outputs": [],
   "source": [
    "# TEST 4: try calling these methods that train with a simple dataset\n",
    "def nn_tanh_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), Tanh(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
    "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
    "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
    "\n",
    "\n",
    "def nn_relu_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=2, lrate=0.005)\n",
    "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
    "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
    "\n",
    "\n",
    "def nn_pred_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
    "    Ypred = nn.forward(X)\n",
    "    return nn.modules[-1].class_fun(Ypred).tolist(), [nn.loss.forward(Ypred, Y)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T00:16:12.806331Z",
     "start_time": "2025-02-16T00:16:12.798649Z"
    },
    "id": "_dx-zM2y3R0z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n",
       "  [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n",
       "  [-8.47337764291184e-12, 2.6227368810847102e-09, 0.00017353185263155828]],\n",
       " [[0.544808855557535, -0.08366117689965663],\n",
       "  [-0.06331837550937104, 0.24078409926389266],\n",
       "  [0.08677202043839037, 0.8360167748667923],\n",
       "  [-0.0037249480614718, 0.0037249480614718]]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_tanh_test()\n",
    "\n",
    "# Expected output:\n",
    "# '''\n",
    "# [[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n",
    "#   [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n",
    "#   [-8.47337764291184e-12, 2.6227368810847106e-09, 0.00017353185263155828]],\n",
    "#  [[0.544808855557535, -0.08366117689965663],\n",
    "#   [-0.06331837550937104, 0.24078409926389266],\n",
    "#   [0.08677202043839037, 0.8360167748667923],\n",
    "#   [-0.0037249480614718, 0.0037249480614718]]]\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T00:16:12.860804Z",
     "start_time": "2025-02-16T00:16:12.852356Z"
    },
    "id": "WmYT9IWk3TQL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 0.5 \tLoss = 0.1491149875248131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n",
       "  [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n",
       "  [-0.0027754917572235106, 0.001212351486908601, -0.0005239629389906042]],\n",
       " [[0.501769700845158, -0.040622022187279644],\n",
       "  [-0.09260786974986723, 0.27007359350438886],\n",
       "  [0.08364438851530624, 0.8391444067898763],\n",
       "  [-0.004252310922204504, 0.004252310922204505]]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_relu_test()\n",
    "\n",
    "# Expected output:\n",
    "# '''\n",
    "# [[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n",
    "#   [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n",
    "#   [-0.0027754917572235106, 0.001212351486908601, -0.0005239629389906042]],\n",
    "#  [[0.501769700845158, -0.040622022187279644],\n",
    "#   [-0.09260786974986723, 0.27007359350438886],\n",
    "#   [0.08364438851530624, 0.8391444067898763],\n",
    "#   [-0.004252310922204504, 0.004252310922204505]]]\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T00:16:12.932995Z",
     "start_time": "2025-02-16T00:16:12.926846Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "uo_woDFh3a2v",
    "outputId": "dc8b59c0-3ae0-447c-ac5c-4a25d010fc46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 0, 0, 0]], [8.565750618357672])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_pred_test()\n",
    "\n",
    "# Expected output:\n",
    "# '''\n",
    "# ([0, 0, 0, 0], [8.56575061835767])+*\n",
    "# '''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
